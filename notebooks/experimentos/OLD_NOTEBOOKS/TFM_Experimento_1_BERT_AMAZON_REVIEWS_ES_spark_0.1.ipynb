{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wIwIlCsolrS",
        "outputId": "b5ac9adb-666e-4521-a338-c1ab140a799e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instalando PySpark 3.2.3 y Spark NLP 4.4.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import platform\n",
        "def setup(pyspark_version=\"3.2.3\", sparknlp_version=\"4.4.2\"):\n",
        "    \"\"\"\n",
        "    Configura el entorno de Google Colab con PySpark y Spark NLP.\n",
        "    :param pyspark_version: versión de PySpark a instalar (opcional)\n",
        "    :param sparknlp_version: versión de Spark NLP a instalar (opcional)\n",
        "    \"\"\"\n",
        "    if platform.system() == 'Linux':\n",
        "        os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "        print(f\"Instalando PySpark {pyspark_version} y Spark NLP {sparknlp_version}\")\n",
        "        if 'NVIDIA' in subprocess.getoutput('nvidia-smi'):\n",
        "            print(\"Actualizando libcudnn8 a 8.1.0 para GPU\")\n",
        "            !apt install -qq --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2 -y &> /dev/null\n",
        "        !pip install --upgrade -q pyspark==$pyspark_version spark-nlp==$sparknlp_version findspark\n",
        "\n",
        "    elif platform.system() == 'Windows':\n",
        "        print(f\"Instalando PySpark {pyspark_version} y Spark NLP {sparknlp_version}\")\n",
        "        !pip install --upgrade -q pyspark==$pyspark_version spark-nlp==$sparknlp_version findspark\n",
        "\n",
        "setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "openjdk version \"11.0.11\" 2021-04-20 LTS\n",
            "OpenJDK Runtime Environment 18.9 (build 11.0.11+9-LTS)\n",
            "OpenJDK 64-Bit Server VM 18.9 (build 11.0.11+9-LTS, mixed mode)\n",
            "\n",
            "Java is installed\n"
          ]
        }
      ],
      "source": [
        "def is_java_installed():\n",
        "    try:\n",
        "        output = subprocess.check_output(['java', '-version'], stderr=subprocess.STDOUT)\n",
        "        print(output.decode())\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return False\n",
        "\n",
        "if is_java_installed():\n",
        "    print(\"Java is installed\")\n",
        "else:\n",
        "    print(\"Java is not installed\")\n",
        "\n",
        "\n",
        "\n",
        "import tarfile\n",
        "import os\n",
        "import shutil\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "def setup_spark_nlp(base_path=r'C:'):\n",
        "    \"\"\"\n",
        "    Sets up Spark NLP with Spark 3.2.3 on Windows.\n",
        "    :param base_path: base path for installation (optional)\n",
        "    \"\"\"\n",
        "    # Step 1: Download and install Adopt OpenJDK 1.8\n",
        "    # This step cannot be automated and must be done manually.\n",
        "\n",
        "    # Step 2: Download pre-compiled Hadoop binaries\n",
        "    hadoop_bin_url = 'https://github.com/cdarlint/winutils/raw/master/hadoop-3.2.0/bin/'\n",
        "    hadoop_bin_dir = os.path.join(base_path, 'hadoop', 'bin')\n",
        "    os.makedirs(hadoop_bin_dir, exist_ok=True)\n",
        "\n",
        "    for file in ['winutils.exe', 'hadoop.dll']:\n",
        "        urllib.request.urlretrieve(hadoop_bin_url + file, os.path.join(hadoop_bin_dir, file))\n",
        "\n",
        "    # Step 3: Download Apache Spark 3.2.3\n",
        "\n",
        "    spark_url = 'https://archive.apache.org/dist/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz'\n",
        "    spark_dir = os.path.join(base_path, 'spark')\n",
        "\n",
        "    with urllib.request.urlopen(spark_url) as response:\n",
        "        with open('spark.tgz', 'wb') as f:\n",
        "            shutil.copyfileobj(response, f)\n",
        "\n",
        "    with tarfile.open('spark.tgz', 'r:gz') as tf:\n",
        "        tf.extractall(spark_dir)\n",
        "\n",
        "    # Step 4: Set environment variables\n",
        "    os.environ['HADOOP_HOME'] = os.path.join(base_path, 'hadoop','bin')\n",
        "    os.environ['SPARK_HOME'] = spark_dir\n",
        "\n",
        "    # Step 5: Add to PATH environment variable\n",
        "    os.environ['PATH'] += os.pathsep + os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
        "    os.environ['PATH'] += os.pathsep + os.path.join(os.environ['SPARK_HOME'], 'bin')\n",
        "\n",
        "    # Step 6: Install Microsoft Visual C++ 2010 Redistributed Package (x64)\n",
        "    # This step cannot be automated and must be done manually.\n",
        "\n",
        "    # Step 7: Create folders C:\\tmp and C:\\tmp\\hive\n",
        "    os.makedirs(os.path.join(base_path, 'tmp', 'hive'), exist_ok=True)\n",
        "\n",
        "    # Change permissions for C:\\tmp and C:\\tmp\\hive\n",
        "    winutils = os.path.join(os.environ['HADOOP_HOME'], 'bin', 'winutils.exe')\n",
        "    os.system(f'{winutils} chmod 777 /tmp/hive')\n",
        "    os.system(f'{winutils} chmod 777 /tmp/')\n",
        "\n",
        "\n",
        "# SOLO EJECUTAR 1 VEZ \n",
        "# setup_spark_nlp(r\"C:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos\\ARCHIVOS_BASE\")\n",
        "\n",
        "\n",
        "\n",
        "# [Environment]::SetEnvironmentVariable(\"HADOOP_HOME\", \"C:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos\\ARCHIVOS_BASE\\hadoop\\bin\", \"User\")\n",
        "# [Environment]::SetEnvironmentVariable(\"SPARK_HOME\", \"C:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos\\ARCHIVOS_BASE\\ARCHIVOS_BASE\\spark\\spark-3.2.3-bin-hadoop3.2\\bin\", \"User\")\n",
        "\n",
        "\n",
        "# $CurrentValue = [Environment]::GetEnvironmentVariable(\"PATH\", \"User\")\n",
        "# [Environment]::SetEnvironmentVariable(\"PATH\", $CurrentValue + \";C:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos\\ARCHIVOS_BASE\\hadoop\\bin\", \"User\")\n",
        "\n",
        "# $CurrentValue = [Environment]::GetEnvironmentVariable(\"PATH\", \"User\")\n",
        "# [Environment]::SetEnvironmentVariable(\"PATH\", $CurrentValue + \";C:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos\\ARCHIVOS_BASE\\spark\\spark-3.2.3-bin-hadoop3.2\\bin\", \"User\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'HADOOP_HOME'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# os.environ[\"HADOOP_HOME\"] = r\"C:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos\\ARCHIVOS_BASE\\hadoop\\bin\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mHADOOP_HOME \u001b[39m\u001b[39m\"\u001b[39m,os\u001b[39m.\u001b[39;49menviron[\u001b[39m\"\u001b[39;49m\u001b[39mHADOOP_HOME\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m      4\u001b[0m \u001b[39m# os.environ[\"SPARK_HOME\"] = r\"C:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos\\ARCHIVOS_BASE\\ARCHIVOS_BASE\\spark\\spark-3.2.3-bin-hadoop3.2\\bin\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSPARK_HOME \u001b[39m\u001b[39m\"\u001b[39m,os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mSPARK_HOME\u001b[39m\u001b[39m\"\u001b[39m])\n",
            "File \u001b[1;32mC:\\Program Files\\Python38\\lib\\os.py:673\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    670\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencodekey(key)]\n\u001b[0;32m    671\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m    672\u001b[0m     \u001b[39m# raise KeyError with the original key value\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodevalue(value)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'HADOOP_HOME'"
          ]
        }
      ],
      "source": [
        "# os.environ[\"HADOOP_HOME\"] = r\"C:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos\\ARCHIVOS_BASE\\hadoop\\bin\"\n",
        "print(\"HADOOP_HOME \",os.environ[\"HADOOP_HOME\"])\n",
        "\n",
        "# os.environ[\"SPARK_HOME\"] = r\"C:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos\\ARCHIVOS_BASE\\ARCHIVOS_BASE\\spark\\spark-3.2.3-bin-hadoop3.2\\bin\"\n",
        "print(\"SPARK_HOME \",os.environ[\"SPARK_HOME\"])\n",
        "\n",
        "\n",
        "# # Add to PATH environment variable\n",
        "# os.environ['PATH'] += os.pathsep + os.path.join(os.environ['HADOOP_HOME'])\n",
        "# os.environ['PATH'] += os.pathsep + os.path.join(os.environ['SPARK_HOME'])\n",
        "\n",
        "print(\"JAVA_HOME \",os.environ[\"JAVA_HOME\"])\n",
        "\n",
        "\n",
        "# abrir enviroment variables desde powershell: & \"$env:SystemRoot\\System32\\SystemPropertiesAdvanced.exe\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Microsoft Windows [Version 10.0.22000.1936]\n",
            "(c) Microsoft Corporation. All rights reserved.\n",
            "\n",
            "(tfmfresh) c:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos>setx PATH %PATH%;\"C:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos\\ARCHIVOS_BASE\\hadoop\\bin\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid syntax. Default option is not allowed more than '2' time(s).\n",
            "Type \"SETX /?\" for usage.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "(tfmfresh) c:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos>setx HADOOP_HOME \"C:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos\\ARCHIVOS_BASE\\hadoop\\bin\"\n",
            "\n",
            "SUCCESS: Specified value was saved.\n",
            "\n",
            "(tfmfresh) c:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos>setx PATH %PATH%;\"C:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos\\ARCHIVOS_BASE\\spark\\spark-3.2.3-bin-hadoop3.2\\bin\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid syntax. Default option is not allowed more than '2' time(s).\n",
            "Type \"SETX /?\" for usage.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "(tfmfresh) c:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos>setx SPARK_HOME  \"C:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos\\ARCHIVOS_BASE\\spark\\spark-3.2.3-bin-hadoop3.2\\bin\"\n",
            "\n",
            "SUCCESS: Specified value was saved.\n",
            "\n",
            "(tfmfresh) c:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos>"
          ]
        }
      ],
      "source": [
        "%%cmd\n",
        "setx PATH %PATH%;\"C:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos\\ARCHIVOS_BASE\\hadoop\\bin\"\n",
        "setx HADOOP_HOME \"C:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos\\ARCHIVOS_BASE\\hadoop\\bin\"\n",
        "setx PATH %PATH%;\"C:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos\\ARCHIVOS_BASE\\spark\\spark-3.2.3-bin-hadoop3.2\\bin\"\n",
        "setx SPARK_HOME  \"C:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\romangu-repo\\notebooks\\TFM\\experimentos\\ARCHIVOS_BASE\\spark\\spark-3.2.3-bin-hadoop3.2\\bin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceW3PQY8o_up",
        "outputId": "dd76fbbd-27d1-44e9-b824-85a848cd1eca"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1105)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1091)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:571)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1633)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$13(SparkContext.scala:510)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$13$adapted(SparkContext.scala:510)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:510)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)\r\n\t... 22 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msparknlp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m spark \u001b[39m=\u001b[39m sparknlp\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSpark NLP version: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(sparknlp\u001b[39m.\u001b[39mversion()))\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mApache Spark version: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(spark\u001b[39m.\u001b[39mversion))\n",
            "File \u001b[1;32mc:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\tfmfresh\\lib\\site-packages\\sparknlp\\__init__.py:289\u001b[0m, in \u001b[0;36mstart\u001b[1;34m(gpu, apple_silicon, aarch64, memory, cache_folder, log_folder, cluster_tmp_dir, params, real_time_output, output_level)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkRealTimeOutput()\u001b[39m.\u001b[39mspark_session\n\u001b[0;32m    288\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 289\u001b[0m     spark_session \u001b[39m=\u001b[39m start_without_realtime_output()\n\u001b[0;32m    290\u001b[0m     \u001b[39mreturn\u001b[39;00m spark_session\n",
            "File \u001b[1;32mc:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\tfmfresh\\lib\\site-packages\\sparknlp\\__init__.py:187\u001b[0m, in \u001b[0;36mstart.<locals>.start_without_realtime_output\u001b[1;34m()\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m         builder\u001b[39m.\u001b[39mconfig(key, value)\n\u001b[1;32m--> 187\u001b[0m \u001b[39mreturn\u001b[39;00m builder\u001b[39m.\u001b[39;49mgetOrCreate()\n",
            "File \u001b[1;32mc:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\tfmfresh\\lib\\site-packages\\pyspark\\sql\\session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    227\u001b[0m     \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    229\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc)\n",
            "File \u001b[1;32mc:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\tfmfresh\\lib\\site-packages\\pyspark\\context.py:392\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 392\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    393\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
            "File \u001b[1;32mc:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\tfmfresh\\lib\\site-packages\\pyspark\\context.py:146\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    144\u001b[0m SparkContext\u001b[39m.\u001b[39m_ensure_initialized(\u001b[39mself\u001b[39m, gateway\u001b[39m=\u001b[39mgateway, conf\u001b[39m=\u001b[39mconf)\n\u001b[0;32m    145\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n\u001b[0;32m    148\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     \u001b[39m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop()\n",
            "File \u001b[1;32mc:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\tfmfresh\\lib\\site-packages\\pyspark\\context.py:209\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment[\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    208\u001b[0m \u001b[39m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc \u001b[39m=\u001b[39m jsc \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_context(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conf\u001b[39m.\u001b[39;49m_jconf)\n\u001b[0;32m    210\u001b[0m \u001b[39m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conf \u001b[39m=\u001b[39m SparkConf(_jconf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39mconf())\n",
            "File \u001b[1;32mc:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\tfmfresh\\lib\\site-packages\\pyspark\\context.py:329\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_initialize_context\u001b[39m(\u001b[39mself\u001b[39m, jconf):\n\u001b[0;32m    326\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[39m    Initialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mJavaSparkContext(jconf)\n",
            "File \u001b[1;32mc:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\tfmfresh\\lib\\site-packages\\py4j\\java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1579\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1580\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1581\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1584\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1585\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1586\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[0;32m   1588\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1589\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
            "File \u001b[1;32mc:\\Users\\RomanGu\\OneDrive - BASF\\Documents\\CODE\\tfmfresh\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1105)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1091)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:571)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1633)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$13(SparkContext.scala:510)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$13$adapted(SparkContext.scala:510)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:510)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)\r\n\t... 22 more\r\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: {}\".format(sparknlp.version()))\n",
        "print(\"Apache Spark version: {}\".format(spark.version))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRZT6k97pDkB"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from sparknlp.base import LightPipeline\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import countDistinct, col\n",
        "import pyspark.sql.functions as F\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import rcParams\n",
        "\n",
        "# Setting up inline plotting and figure format\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "# Setting up seaborn style and color palette\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "\n",
        "# Setting up figure size for plots\n",
        "rcParams['figure.figsize'] = 12, 8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvaPZNSOqRfS"
      },
      "outputs": [],
      "source": [
        "def experimento(tipo: str, idioma: str):\n",
        "    \"\"\"\n",
        "    Configura y devuelve los parámetros para un experimento.\n",
        "\n",
        "    Parámetros\n",
        "    ----------\n",
        "    tipo : str\n",
        "        El tipo de experimento a realizar. Debe ser \"AMAZON\" o \"AGNEWS\".\n",
        "    idioma : str\n",
        "        El idioma del modelo pre-entrenado a utilizar. Debe ser \"EN\" o \"ES\".\n",
        "\n",
        "    Devoluciones\n",
        "    -------\n",
        "    int\n",
        "        El tamaño del lote a utilizar en el experimento.\n",
        "    int\n",
        "        El número de épocas a utilizar en el experimento.\n",
        "    float\n",
        "        La tasa de aprendizaje a utilizar en el experimento.\n",
        "    str\n",
        "        El nombre del modelo pre-entrenado a utilizar en el experimento.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    experiment_params = {\n",
        "        \"AMAZON\": (16, 2, 5e-5),\n",
        "        \"AGNEWS\": (32, 4, 3e-5)\n",
        "    }\n",
        "    \n",
        "    model_names = {\n",
        "        \"ES\": 'bert_base_multilingual_cased',\n",
        "        \"EN\": 'bert_uncased_L-4_H-256_A-4_squad2',\n",
        "    }\n",
        "    \n",
        "    if tipo.upper() not in experiment_params:\n",
        "        print(\"elige experimento\")\n",
        "        return\n",
        "    \n",
        "    if idioma.upper() not in model_names:\n",
        "        print(\"modelo no elegido\")\n",
        "        return\n",
        "    \n",
        "    BATCH_SIZE, EPOCHS, LEARNING_RATE = experiment_params[tipo.upper()]\n",
        "    PRE_TRAINED_MODEL_NAME = model_names[idioma.upper()]\n",
        "    \n",
        "    return BATCH_SIZE, EPOCHS, LEARNING_RATE, PRE_TRAINED_MODEL_NAME\n",
        "\n",
        "\n",
        "BATCH_SIZE, EPOCHS, LEARNING_RATE, PRE_TRAINED_MODEL_NAME = experimento(tipo = \"AMAZON\", idioma =\"es\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "69WyiC--FogB"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# En spark NLP - tamaños para los BERT models\n",
        "\n",
        "Modelos elegidos:\n",
        "* Para el modelo de respuesta en inglés [bert_uncased_L-4_H-256_A-4_squad2](https://sparknlp.org/2022/06/02/bert_qa_bert_uncased_L_4_H_256_A_4_squad2_en_3_0.html) \n",
        "  * el más parecido a [google/bert_uncased_L-4_H-256_A-4](https://huggingface.co/google/bert_uncased_L-4_H-256_A-4)\n",
        "* para el modelo de clasificación en español si está el mismo modelo de HugginFace  [bert_base_multilingual_cased](https://sparknlp.org/2021/05/20/bert_base_multilingual_cased_xx.html)\n",
        "\n",
        "<!-- Original values\n",
        "* PRE_TRAINED_MODEL_NAME = 'bert_base_cased'  **389.1 MB**\n",
        "* PRE_TRAINED_MODEL_NAME = 'sent_small_bert_L8_512' **149.1 MB**\n",
        "* PRE_TRAINED_MODEL_NAME = 'small_bert_L4_256' **40.5 MB**\n",
        "\n",
        "Para sentence embeddings.\n",
        "SENTENCE_PRE_TRAINED_MODEL_NAME = 'sent_bert_base_cased'\n",
        "SENTENCE_PRE_TRAINED_MODEL_NAME = 'sent_small_bert_L8_512' **149.1 MB** -->\n",
        "\n",
        "Fuente: [NLP MODELS EN SPARK-NLP](https://nlp.johnsnowlabs.com/models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjSNtU_8qlAZ",
        "outputId": "636d065e-3249-4cf0-b953-313459da42f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BATCH_SIZE: 16\n",
            "LEARNING_RATE: 5e-05\n",
            "EPOCHS: 2\n",
            "PRE_TRAINED_MODEL_NAME: bert_base_multilingual_cased\n",
            "PRE_TRAINED_MODEL_NAME FOR SENTENCES: sent_bert_base_multilingual_cased\n"
          ]
        }
      ],
      "source": [
        "# Para las Sentence embeddings el nombre del moldelo lleva un prefijo \"sent_\"\n",
        "SENTENCE_PRE_TRAINED_MODEL_NAME = 'sent_'+PRE_TRAINED_MODEL_NAME\n",
        "\n",
        "# Random Seed \n",
        "RANDOM_SEED = 42\n",
        "\n",
        "\n",
        "print(f\"BATCH_SIZE: {BATCH_SIZE}\")\n",
        "print(f\"LEARNING_RATE: {LEARNING_RATE}\")\n",
        "print(f\"EPOCHS: {EPOCHS}\")\n",
        "print(f\"PRE_TRAINED_MODEL_NAME: {PRE_TRAINED_MODEL_NAME}\")\n",
        "print(f\"PRE_TRAINED_MODEL_NAME FOR SENTENCES: {SENTENCE_PRE_TRAINED_MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThqjEH_FqoTA",
        "outputId": "ed3e9bea-011c-4447-8a7a-5c2d506f8314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting awscli\n",
            "  Downloading awscli-1.27.137-py3-none-any.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore==1.29.137 (from awscli)\n",
            "  Downloading botocore-1.29.137-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: docutils<0.17,>=0.10 in /usr/local/lib/python3.10/dist-packages (from awscli) (0.16)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from awscli)\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyYAML<5.5,>=3.10 (from awscli)\n",
            "  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting colorama<0.4.5,>=0.2.5 (from awscli)\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting rsa<4.8,>=3.1.2 (from awscli)\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore==1.29.137->awscli)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore==1.29.137->awscli) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore==1.29.137->awscli) (1.26.15)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.29.137->awscli) (1.16.0)\n",
            "Building wheels for collected packages: PyYAML\n",
            "  Building wheel for PyYAML (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.4.1-cp310-cp310-linux_x86_64.whl size=45658 sha256=7001e24029e7a01c21975023c61d2e6fe060c8dd80ea696b2a9805a405cb1bdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/0d/22/696ee92245ad710f506eee79bb05c740d8abccd3ecdb778683\n",
            "Successfully built PyYAML\n",
            "Installing collected packages: rsa, PyYAML, jmespath, colorama, botocore, s3transfer, awscli\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "Successfully installed PyYAML-5.4.1 awscli-1.27.137 botocore-1.29.137 colorama-0.4.4 jmespath-1.0.1 rsa-4.7.2 s3transfer-0.6.1\n",
            "download: s3://amazon-reviews-ml/json/test/dataset_es_test.json to sample_data/dataset_es_test.json\n",
            "download: s3://amazon-reviews-ml/json/train/dataset_es_train.json to sample_data/dataset_es_train.json\n"
          ]
        }
      ],
      "source": [
        "!pip install awscli -qq\n",
        "\n",
        "root_folder = \"./sample_data/\"\n",
        "\n",
        "if not os.path.isfile(os.path.join(root_folder,'dataset_es_test.json')):\n",
        "    !aws s3 cp s3://amazon-reviews-ml/json/test/dataset_es_test.json ./sample_data/ --no-sign-request\n",
        "if not os.path.isfile(os.path.join(root_folder,'dataset_es_train.json')):\n",
        "    !aws s3 cp s3://amazon-reviews-ml/json/train/dataset_es_train.json ./sample_data/ --no-sign-request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hxkq6zmqs-f",
        "outputId": "0eec35c1-d083-4e5f-f402-b8092a32dfb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anscombe.json\t\t      dataset_es_test.json   mnist_train_small.csv\n",
            "california_housing_test.csv   dataset_es_train.json  README.md\n",
            "california_housing_train.csv  mnist_test.csv\n"
          ]
        }
      ],
      "source": [
        "os.listdir(root_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-gFeeluqvZL"
      },
      "outputs": [],
      "source": [
        "traindata = os.path.join(root_folder, \"dataset_es_train.json\")\n",
        "testdata = os.path.join(root_folder, \"dataset_es_test.json\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if platform.system() == 'Linux':\n",
        "    !head {traindata}\n",
        "else:\n",
        "    !powershell -command \"gc {traindata} | select -first 10 # head\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c0iy1abqyni",
        "outputId": "ca21af00-a600-4d82-9656-a74683359ef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño total  200000 2\n",
            "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|stars|description                                                                                                                                                                                                                                                                                                                                              |\n",
            "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1    |Nada bueno se me fue ka pantalla en menos de 8 meses y no he recibido respuesta del fabricante                                                                                                                                                                                                                                                           |\n",
            "|1    |Horrible, nos tuvimos que comprar otro porque ni nosotros que sabemos inglés, ni un informático, después de una hora fue capaz de instalarlo                                                                                                                                                                                                             |\n",
            "|1    |Te obligan a comprar dos unidades y te llega solo una y no hay forma de reclamar, una autentica estafa, no compreis!!                                                                                                                                                                                                                                    |\n",
            "|1    |No entro en descalificar al vendedor, solo puedo decir que tras dos meses de espera.... sigo sin el producto y tuve que contactar con Amazon para reclamar su reembolso. Amazon un 10 . Se hace cargo del problema, pero yo e desembolsado mi dinero y en dos meses me lo devuelven Perdida de tiempo TOTAL. Sin palabras. Y Ustedes deciden             |\n",
            "|1    |Llega tarde y co la talla equivocada                                                                                                                                                                                                                                                                                                                     |\n",
            "|1    |Jamás me llegó y el vendedor nunca contacto conmigo a pesar de intentarlo 2 veces                                                                                                                                                                                                                                                                        |\n",
            "|1    |El paraguas es de muy mala calidad,da la sensación que se va a romper cuando lo abres. Es muy cutre.                                                                                                                                                                                                                                                     |\n",
            "|1    |Tuve que devolverla porque al ser triangular no se agarra de forma cómoda para la escritura. Bonita, calidad-precio aceptable, pero poco funcional.                                                                                                                                                                                                      |\n",
            "|1    |Estoy esperando despues de protestar varias veces pero veo que no hay solucion y no haceis caso me dicen de un reembolso pero yo quiero el auricular                                                                                                                                                                                                     |\n",
            "|1    |Defectuoso. En apariencia muy bien producto, pero al tercer uso del vaso mezclador más grande ha dejado de funcionar. Intentaba hacer un puré, nada que requiera una fuerza excesiva y las cuchillas dejaron de girar. Una pena porque es una herramienta, en principio, muy funcional. Al final lo “barato” sale caro. Tres meses de vida. Decepcionada.|\n",
            "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "df = spark.read.json(traindata).select(\n",
        "    col(\"stars\"), col(\"review_body\").alias(\"description\")\n",
        ")\n",
        "df_testdata = spark.read.json(testdata).select(\n",
        "    col(\"stars\"), col(\"review_body\").alias(\"description\")\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Tamaño total \", df.count(), len(df.columns)\n",
        ")  # dataframe shape only exists in recent versions\n",
        "\n",
        "df.show(10, truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NihF6SH0tNHS"
      },
      "source": [
        "# Creando las categorías del sentiment en base al numero de estrellas:\n",
        "Este fragmento de código crea una nueva columna llamada \"sentiment\" en dos DataFrames `df` y `df_testdata`. La columna \"sentiment\" se llena con valores basados en la columna \"stars\" de cada DataFrame.\n",
        "\n",
        "Para cada fila en el DataFrame `df`, si el valor en la columna \"stars\" está entre 0 y 2 (exclusivo), el valor correspondiente en la columna \"sentiment\" se establece en \"negative\". Si el valor en la columna \"stars\" es igual a 3, el valor correspondiente en la columna \"sentiment\" se establece en \"neutral\". Para todos los demás valores en la columna \"stars\", el valor correspondiente en la columna \"sentiment\" se establece en \"positive\".\n",
        "\n",
        "El mismo proceso se aplica al DataFrame `df_testdata`.\n",
        "\n",
        "Esto se logra utilizando la función `when` de spark, que toma una lista de condiciones y una lista de valores a seleccionar cuando se cumple cada condición. Si ninguna de las condiciones se cumple para un elemento dado, se selecciona el valor especificado por el parámetro `otherwise`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cM3zbGRZrSSA",
        "outputId": "1e56d0dc-29da-4916-a1c2-056f4b78d930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
            "|stars|description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |sentiment|\n",
            "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
            "|1    |no me llego el articulo me lo mando por correos normal sin seguimiento y nunca me llego tota un desastre                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |negative |\n",
            "|1    |la mensajería horrible, no compro mas                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |negative |\n",
            "|1    |Estoy muy decepcionado con el vendedor ya que el pedido no me llegó a tiempo y no cumplió los plazos de envío y era una cosa que necesitaba urgente. Para colmo me pongo en contacto con el vendedor y no da señales. No recomiendo nada                                                                                                                                                                                                                                                                                                                                                  |negative |\n",
            "|1    |Mi valoración no es sobre el producto sino sobre AMAZON. Ofrecéis el producto a 299€ y tras varios días me devolvéis el dinero porque os habéis equivocado en el anuncio, según vosotros, ahora es 399€. Es la primera vez que me ocurre esto. Cuando he comprado en cualquier sitio y el precio marcado no se correspondía con el valor de caja siempre me lo han vendido con el precio marcado. Es inverosímil lo ocurrido, pero la ultima palabra me la dará la oficina del consumidor                                                                                                 |negative |\n",
            "|1    |Pues tenía interés en este libro y probé la versión kindle. se abre la portada pero nada más. parece una mala broma pero me iba a gastar el dedo de tanto tratar que pasara de página de alguna forma. No puedo valorar nada de nada.                                                                                                                                                                                                                                                                                                                                                     |negative |\n",
            "|1    |Compre este teclado al ver sus buenos comentarios y apariencia pero al llegar a casa, lo puse a cargar y al quererlo conectar al iPad ni encendía ni hacía nada una verdadera pena                                                                                                                                                                                                                                                                                                                                                                                                        |negative |\n",
            "|1    |Sigue sin llegar después de meses                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |negative |\n",
            "|1    |No sirve para nada, es malo y se rompe y se despega                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |negative |\n",
            "|1    |Todavía espero que me llegue despues dw una semana de retraso, pesimo trato por parte del vendedor, no reembolsa importa aun sin haberlo recibido.                                                                                                                                                                                                                                                                                                                                                                                                                                        |negative |\n",
            "|1    |La peor cámara que he tenido en mis manos. Dos veces la he tenido que reemplazar al primer día de uso. Al final con la mitad de accesorios comprados la he tenido que devolver y no comprarla más. La cámara se queda congelada. Se cargaba la batería cuando ella quería y se descargaba sola sin usarla. Problemas de actualización, vamos si quieres tirar el dinero cómprate está cámara. Una gran decepción. Esperaba que está cámara funcionara bien pero ha sido una pesadilla. Amazon me dijo que la iba a quitar de la venta por que había muchísimas devoluciones de esta cámara|negative |\n",
            "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = df.withColumn(\n",
        "    \"sentiment\",\n",
        "    when(df[\"stars\"].between(0, 2), \"negative\")\n",
        "    .when(df[\"stars\"] == 3, \"neutral\")\n",
        "    .otherwise(\"positive\")\n",
        ")\n",
        "\n",
        "df_testdata = df_testdata.withColumn(\n",
        "    \"sentiment\",\n",
        "    when(df_testdata[\"stars\"].between(0, 2), \"negative\")\n",
        "    .when(df_testdata[\"stars\"] == 3, \"neutral\")\n",
        "    .otherwise(\"positive\")\n",
        ")\n",
        "\n",
        "df_testdata.show(10,truncate=90)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCz7S_gyrkog"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"category_encoded\")\n",
        "indexer_model = indexer.fit(df)\n",
        "df = indexer_model.transform(df)\n",
        "df_testdata = indexer_model.transform(df_testdata)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbNFcO8iCOgd",
        "outputId": "6ae22d23-df6f-4192-d7d2-2fd78164da36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+-----+\n",
            "|sentiment|count|\n",
            "+---------+-----+\n",
            "| negative|40000|\n",
            "|  neutral|40000|\n",
            "| positive|40000|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# igualamos el numero de ocurrencias por categoria para no tener bias.\n",
        "df_temp = df.filter(col(\"sentiment\") == \"negative\").limit(40000) \\\n",
        "    .union(df.filter(col(\"sentiment\") == \"neutral\").limit(40000)) \\\n",
        "    .union(df.filter(col(\"sentiment\") == \"positive\").limit(40000))\n",
        "\n",
        "df_testdata_temp = df_testdata.filter(col(\"sentiment\") == \"negative\").limit(1000) \\\n",
        "    .union(df_testdata.filter(col(\"sentiment\") == \"neutral\").limit(1000)) \\\n",
        "    .union(df_testdata.filter(col(\"sentiment\") == \"positive\").limit(1000))\n",
        "\n",
        "# reemplazamos los dataset uno con los datos, df para train y val, y df_testdata para la prueba \"test\"\n",
        "df = df_temp\n",
        "df_testdata = df_testdata_temp\n",
        "\n",
        "df.groupBy(\"sentiment\").count().filter(col(\"count\") > 1).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "zI-1BlhbC3pT",
        "outputId": "4df76ddd-5c5c-4644-ab55-cde85878109e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sentiment_counts = df.groupBy(\"sentiment\").count().toPandas()\n",
        "\n",
        "colors = [ \"#7f7f7f\", \"#bcbd22\", \"#17becf\", \"#1a55FF\", \"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\", \"#8c564b\", \"#e377c2\", ]\n",
        "\n",
        "plt.bar(sentiment_counts[\"sentiment\"], sentiment_counts[\"count\"],color = colors)\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Sentiment Counts\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k-ywY0uaHCf1"
      },
      "source": [
        "Explicación del naming en los modelos Bert de Spark NLP. \n",
        "* \"L\" indica qué capa de agrupación (pooling layer) se utiliza al producir las incrustaciones (embeddings) \n",
        "* \"H\" significa la dimensión de las embeddings devueltas.\n",
        "\n",
        "Por ejemplo, si utilizamos `BertSentenceEmbeddings` con el valor `'sent_small_bert_L8_512'` significa que simplemente cargamos Bert Sentence Embeddings Small con dimensión L8 y 512 y lo usamos en lugar de USE. Como puede ver, es casi 8 veces más pequeño que el tamaño de USE con el poder de Bert.\n",
        "\n",
        "El modelo se puede utilizar de manera offline si se desea, pero habría que descargarlo previamente. Para no incrementar el tamaño del repositorio dejamos a SPARK_NLP que lo baje automáticamente.\n",
        "\n",
        "\n",
        "En el contexto del Procesamiento del Lenguaje Natural (NLP), USE significa **Universal Sentence Encoder**. Es una herramienta desarrollada por Google que convierte una cadena de palabras en vectores de 512 dimensiones. Estos vectores capturan el significado semántico de la secuencia de palabras en una oración y, por lo tanto, pueden usarse como entradas para otras tareas de NLP posteriores, como clasificación, medición de similitud semántica, etc ."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nNJt-pLmJhPV"
      },
      "source": [
        "A continuación se está construyendo un pipeline de procesamiento de Spark NLP. Primero, se está creando un `DocumentAssembler` para convertir la columna \"review_body\" en un documento y almacenarlo en la columna \"document\". Luego, se está creando un `Tokenizer` para dividir el documento en tokens y almacenarlos en la columna \"token\".\n",
        "\n",
        "Después, se está utilizando `BertEmbeddings` para crear incrustaciones (embeddings) de palabras utilizando un modelo pre-entrenado especificado por `PRE_TRAINED_MODEL_NAME`. Estas incrustaciones se almacenan en la columna \"embeddings\".\n",
        "\n",
        "A continuación, se está utilizando `SentenceEmbeddings` para calcular las incrustaciones de oraciones a partir de las incrustaciones de palabras utilizando un promedio y almacenarlas en la columna \"sentence_embeddings\".\n",
        "\n",
        "Finalmente, se está creando un clasificador `ClassifierDLApproach` que utiliza las incrustaciones de oraciones como entrada para clasificar el sentimiento del texto. El clasificador se entrena durante un número especificado de épocas con una tasa de aprendizaje y tamaño de lote especificados.\n",
        "\n",
        "Todo el pipeline se construye utilizando estas etapas y se almacena en la variable `bert_clf_pipeline`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoiAvy9yDRTy",
        "outputId": "2bb14b6b-5eb5-4748-f8b4-c1417b0821bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bert_base_multilingual_cased download started this may take some time.\n",
            "Approximate size to download 636.6 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"description\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "    \n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "# BertEmbeddings utiliza los datos de una red neuronal entrenada previamente (el nombre del modelo) para crear codificar las palabras que contiene cada texto \n",
        "# convirtiendo a vectores dichas palabras. (codificandolas) AQUI USE AL PRINCIPIO 'small_bert_L4_256'\n",
        "bert_embeddings = BertEmbeddings().pretrained(name=PRE_TRAINED_MODEL_NAME, lang='xx') \\\n",
        "    .setInputCols([\"document\",'token'])\\\n",
        "    .setOutputCol(\"embeddings\")\n",
        "#\n",
        "embeddingsSentence = SentenceEmbeddings() \\\n",
        "    .setInputCols([\"document\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"sentence_embeddings\") \\\n",
        "    .setPoolingStrategy(\"AVERAGE\")\n",
        "# ClassifierDLApproach: ClassifierDL Utiliza el State-of-the-art de las Universal Sentence Encoder como input para iniciar clasificaciones de texto.\n",
        "# es un clasificador que contiene casi 100 clases de tensorflow, usa modelos de deep learning DNN\n",
        "# LR 5e-3f BATCHSIZE 8\n",
        "classsifierdl = ClassifierDLApproach()\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"class\")\\\n",
        "    .setLabelColumn(\"sentiment\")\\\n",
        "    .setMaxEpochs(EPOCHS)\\\n",
        "    .setLr(LEARNING_RATE)\\\n",
        "    .setBatchSize(BATCH_SIZE)\\\n",
        "    .setEnableOutputLogs(True)\n",
        "    #.setOutputLogsPath('logs')\n",
        "\n",
        "bert_clf_pipeline = Pipeline(stages=[\n",
        "    document_assembler, \n",
        "    tokenizer,\n",
        "    bert_embeddings,\n",
        "    embeddingsSentence,\n",
        "    classsifierdl\n",
        "])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "32vc5FNlKH02"
      },
      "source": [
        "# Creación del train y val datasets\n",
        "\n",
        "HAY QUE HACER ONE HOT ENCODING PARA PODER METERLO EN EL BERT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDE5X3RHKq1s",
        "outputId": "74d6682a-b35c-49ba-ce03-f10bcd7d1595"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "84201\n",
            "35799\n"
          ]
        }
      ],
      "source": [
        "df_train, df_eval = df.randomSplit([0.7, 0.3], seed=RANDOM_SEED)\n",
        "for dataset in (df_train, df_eval):\n",
        "  print(dataset.count())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cxazd96MT_R1"
      },
      "source": [
        "# Ajuste del modelo (FIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4vZXVLKISOc"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "bert_Model = bert_clf_pipeline.fit(df_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7nM5fJYwURd-"
      },
      "source": [
        "# Realización de predicciones en el conjunto de prueba\n",
        "1. primero con una frase\n",
        "2. con el dataset de test "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBtSbpqcUUdA"
      },
      "outputs": [],
      "source": [
        "light_model = LightPipeline(bert_Model)\n",
        "\n",
        "light_result = light_model.annotate(\"Este producto es un poco asqueroso, no me ha gustado nada\")\n",
        "print(light_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRmd6kl7cb1C"
      },
      "outputs": [],
      "source": [
        "predict_bert = bert_Model.transform(df_testdata)\n",
        "\n",
        "predict_bert\\\n",
        "  .select(\"sentiment\",\"description\",\"class.result\")\\\n",
        "  .withColumn(\"prediccion\", \n",
        "              when(df.result[0] == 0, ['negative'])\n",
        "              .when(df.result[0] == 1, ['neutral'])\n",
        "              .when(df.result[0] == 2, ['positive'])).show(10,truncate=80)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t0JuDphtc-MN"
      },
      "source": [
        "# Obtenemos el rendimiento del modelo para comparar con la opcion de USE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pxil8JWrc-kx"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_pandas_predict_bert = predict_bert.toPandas()\n",
        "\n",
        "\n",
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "print(classification_report(df_pandas_predict_bert.sentiment, df_pandas_predict_bert.result,target_names=class_names))\n",
        "print(accuracy_score(df_pandas_predict_bert.sentiment, df_pandas_predict_bert.result))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3mpE9u1FfGfQ"
      },
      "source": [
        "# Dreamos la matriz de confusion\n",
        "1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX60AZEifIjw"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True sentiment')\n",
        "  plt.xlabel('Predicted sentiment');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXgHiSLafl1Z"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(df.sentiment, df.result)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fFNLcGt3f4sx"
      },
      "source": [
        "Pruebas de predicción\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o57llAZgAe6"
      },
      "outputs": [],
      "source": [
        "light_model.annotate(\n",
        "    \"Me encanta, lo recomendaría al 100%, no se como no se lo han comprado antes.\"\n",
        ")[\"class\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BcJ3cVt_b-nT"
      },
      "source": [
        "Enlaces relacionados bibliografía organizada en formato markdown:\n",
        "\n",
        "- [Annotators SparkNLP](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/5.Text_Classification_with_ClassifierDL.ipynb)\n",
        "- [Opciones de preprocesado](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb)\n",
        "- [Clasificación de texto en Spark NLP con Bert y Universal Sentence Encoders](https://towardsdatascience.com/text-classification-in-spark-nlp-with-bert-and-universal-sentence-encoders-e644d618ca32)\n",
        "- [Cómo instalar Spark NLP offline](https://nlp.johnsnowlabs.com/docs/en/install#offline)\n",
        "- [Predicción de reseñas de películas con Bert en TF Hub (Python)](https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=OjwJ4bTeWXD8)\n",
        "- [Ajuste fino de Bert con Cloud TPUs (Python)](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "- [Repositorio de Bert en GitHub](https://github.com/google-research/bert)\n",
        "- [Ventajas de Spark NLP vs Spacy](https://towardsdatascience.com/introduction-to-spark-nlp-foundations-and-basic-components-part-i-c83b7629ed59)\n",
        "- [Explicación de Bert en diapositivas](http://web.stanford.edu/class/cs224n/slides/Jacob_Devlin_BERT.pdf)\n",
        "- [Información sobre la disponibilidad de GPU en Colab](https://research.google.com/colaboratory/faq.html#gpu-availability)\n",
        "- [Configuracion de spark NLP](https://github.com/JohnSnowLabs/spark-nlp/blob/master/README.md#quick-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcgLyJxRbpVA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
