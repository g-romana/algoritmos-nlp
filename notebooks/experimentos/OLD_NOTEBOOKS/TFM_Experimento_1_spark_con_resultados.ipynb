{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq transformers"
      ],
      "metadata": {
        "id": "kKV4sFXtNG5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código carga los conjuntos de datos de entrenamiento y prueba desde los archivos CSV utilizando Pandas. Luego codifica las etiquetas categóricas como números enteros utilizando un objeto LabelEncoder. Se define una clase personalizada para representar los conjuntos de datos como objetos Dataset de PyTorch. Esta clase tokeniza el texto utilizando el tokenizer BERT y devuelve los tensores de entrada para el modelo junto con la etiqueta correspondiente. Luego se crean objetos Dataset para los conjuntos de datos de entrenamiento y prueba y se crean DataLoaders para ellos. El resto del código es similar al ejemplo anterior.\n",
        "\n",
        "Una vez termina el entrenamiento del modelo, se guarda un csv con los estadísticos importantes para su posterior comparación:\n",
        "- datetime de la realizacion de la prueba\n",
        "- tamaño de la muestra\n",
        "- modelo bert utilizado.\n",
        "- Tiempo de entrenamiento\n",
        "- memoria utilizada\n",
        "- Si ha utilizado GPU\n",
        "- Uso de memoria de GPU\n",
        "- estadísticos de precisión del accuracy_score."
      ],
      "metadata": {
        "id": "qgMhK7IHNHcW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "0G4i1uoxKV6F",
        "outputId": "91151704-e3fb-4186-ab28-2b22c6be68c1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ecd266680ef0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "\n",
        "# Definir el nombre del modelo BERT utilizado\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "\n",
        "# Cargar el tokenizer y el modelo pre-entrenado de BERT\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(bert_model_name, num_labels=4)\n",
        "\n",
        "\n",
        "\n",
        "train_data = pd.read_csv (\"./sample_data/news_category_train.csv\",sep=\",\",engine=\"c\",error_bad_lines=False)\n",
        "print(train_data.head())\n",
        "\n",
        "test_data = pd.read_csv (\"./sample_data/news_category_test.csv\",sep=\",\",engine=\"c\",error_bad_lines=False)\n",
        "\n",
        "\n",
        "# Instalar Spark NLP\n",
        "!pip install spark-nlp==3.3.1\n",
        "\n",
        "# Importar las librerías necesarias\n",
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import time\n",
        "import psutil\n",
        "from datetime import datetime\n",
        "\n",
        "# Definir el nombre del modelo BERT utilizado\n",
        "bert_model_name = 'bert_base_cased'\n",
        "\n",
        "# Cargar los conjuntos de datos de entrenamiento y prueba desde los archivos CSV\n",
        "base_url = \"https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data\"\n",
        "train_url = base_url+\"/news_category_train.csv\"\n",
        "test_url = base_url+\"/news_category_test.csv\"\n",
        "\n",
        "urllib.request.urlretrieve(train_url, \"./sample_data/news_category_train.csv\")\n",
        "urllib.request.urlretrieve(test_url, \"./sample_data/news_category_test.csv\")\n",
        "\n",
        "\n",
        "train_data = spark.read.option(\"header\", True).csv(\"./sample_data/news_category_train.csv\")\n",
        "train_data.show(truncate=False)\n",
        "test_data = spark.read.option(\"header\", True).csv(\"./sample_data/news_category_test.csv\")\n",
        "\n",
        "# Codificar las etiquetas categóricas como números enteros\n",
        "label_indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\").fit(train_data)\n",
        "train_data = label_indexer.transform(train_data)\n",
        "test_data = label_indexer.transform(test_data)\n",
        "\n",
        "# Definir el pipeline de procesamiento de texto utilizando Spark NLP\n",
        "document_assembler = DocumentAssembler().setInputCol(\"description\").setOutputCol(\"document\")\n",
        "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
        "bert_embeddings = BertEmbeddings.pretrained(bert_model_name, \"en\").setInputCols([\"document\", \"token\"]).setOutputCol(\"embeddings\")\n",
        "embeddings_sentence = SentenceEmbeddings().setInputCols([\"document\", \"embeddings\"]).setOutputCol(\"sentence_embeddings\").setPoolingStrategy(\"AVERAGE\")\n",
        "classifier_dl = ClassifierDLApproach().setInputCols([\"sentence_embeddings\"]).setOutputCol(\"prediction\").setLabelColumn(\"label\").setMaxEpochs(5).setEnableOutputLogs(True)\n",
        "nlp_pipeline = Pipeline(stages=[document_assembler, tokenizer, bert_embeddings, embeddings_sentence, classifier_dl])\n",
        "\n",
        "# Entrenar el modelo y guardar estadísticas en un archivo CSV\n",
        "with open('stats.csv', 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['experiment_name','datetime', 'sample_size', 'bert_model', 'training_time', 'memory_usage', 'gpu_usage', 'gpu_memory_usage', 'accuracy'])\n",
        "\n",
        "    start_time = time.time()\n",
        "    nlp_model = nlp_pipeline.fit(train_data)\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    memory_usage = psutil.virtual_memory().percent\n",
        "\n",
        "    gpu_usage = torch.cuda.is_available()\n",
        "    gpu_memory_usage = torch.cuda.memory_allocated() if gpu_usage else 0\n",
        "\n",
        "    # Evaluar el modelo en el conjunto de datos de prueba\n",
        "    predictions = nlp_model.transform(test_data)\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "    accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "    writer.writerow(['Spark NLP - AG News',datetime.now(), train_data.count(), bert_model_name, training_time, memory_usage, gpu_usage, gpu_memory_usage, accuracy])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crea una función `predict` para hacer una predicción con el modelo BERT que se ha entrenado utilizando un texto de entrada X. La función toma como argumentos el modelo y el texto de entrada"
      ],
      "metadata": {
        "id": "NaWb1vffMprF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, text):\n",
        "    # Crear un DataFrame de Spark con el texto de entrada\n",
        "    data = spark.createDataFrame([(text,)], ['description'])\n",
        "\n",
        "    # Aplicar el pipeline de procesamiento de texto al texto de entrada\n",
        "    result = model.transform(data)\n",
        "\n",
        "    # Obtener la etiqueta predicha\n",
        "    predicted_label = result.select('prediction').collect()[0]['prediction']\n",
        "    predicted_label = label_indexer.labels[int(predicted_label)]\n",
        "\n",
        "    return predicted_label"
      ],
      "metadata": {
        "id": "_n2fm97kMpAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## prediccion una palabra"
      ],
      "metadata": {
        "id": "n3Vh5ptAMwh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'aftermatch everything went dark, 1 death 2 injuries'\n",
        "predicted_label = predict(nlp_model, text)\n",
        "print(f'Texto: {text}')\n",
        "print(f'Etiqueta predicha: {predicted_label}')"
      ],
      "metadata": {
        "id": "MWa0xw2IMvsZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}